{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ndCIgtebCQO"
   },
   "source": [
    "# Notebook to fine-tune Segment Anything (SAM)\n",
    "This notebook goes together with this [medium post](https://medium.com/p/34514ee811bb/edit). The article provides a high-level overview of the concepts of how to fine-tune SAM. Read it first (if you haven't already), it will help to understand the colab. The structure will be:\n",
    "- 00 Installation and define Variables\n",
    "- 01 SAM without fine-tuning\n",
    "- 02 Define functions and classes for fine-tuning\n",
    "- 03 Plan for fine-tuning\n",
    "- 04 Define models and classes for fine-tuning\n",
    "- 05 Wrap up\n",
    "\n",
    "\n",
    "\n",
    "## 00 Installing packages and defining variables\n",
    "Start with the boring part. The code in this section will import everything you need and define the global variables. If you use your data, we also define this here, but I'll also provide a small dataset. So in summary:\n",
    "- install packages\n",
    "- define variables\n",
    "- adjust for your data\n",
    "\n",
    "Let's go!\n",
    "\n",
    "## Acknowledgments:\n",
    "\n",
    "I thank:\n",
    "- [Luca Medeiros](https://github.com/luca-medeiros/lightning-sam/tree/main/lightning_sam) whos code (even I could not get to run it on our cluster) helped me immensly in understanding SAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzlnytrWlHS5"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R2mNb494bCQS"
   },
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry, SamPredictor\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import toml\n",
    "import os\n",
    "import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrvbU3KFlLil"
   },
   "source": [
    "### Define variables\n",
    "In this section, we'll define global variables. Whenever I put TODO as a comment, you might want to change my path to your own data, or adjust model parameters, etc.\n",
    "\n",
    "To use data on Google Drive, we first need to mount Google Drive like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cm2M1Pqxl8nv",
    "outputId": "d07ec599-9c6b-4633-f843-1f96b3688f8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.types import Device\n",
    "#data\n",
    "global TRAIN\n",
    "global TEST\n",
    "global ANNOT\n",
    "\n",
    "# TODO: Put your path here !!!!\n",
    "TRAIN = os.path.join(r\"E:\\Random Python Scripts\\Tata HaxS\\Car dentss.v1i.coco-segmentation\\train\")\n",
    "TEST = os.path.join(r\"E:\\Random Python Scripts\\Tata HaxS\\Car dentss.v1i.coco-segmentation\\test\")\n",
    "ANNOT = \"_annotations.coco.json\"\n",
    "#model\n",
    "global BATCH_SIZE\n",
    "global EPOCHS\n",
    "global LR\n",
    "global WEIGHT_DECAY\n",
    "global DEVICE\n",
    "\n",
    "# TODOD adjust, if needed!!!!\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 0.0005\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# loss\n",
    "global ALPHA\n",
    "global GAMMA\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "EVALUATE=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOhpwnMj986d"
   },
   "source": [
    "### Take a first look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rc-Jw5-lj5DO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[127, 129, 129],\n",
       "        [128, 130, 130],\n",
       "        [130, 132, 132],\n",
       "        ...,\n",
       "        [194, 194, 194],\n",
       "        [198, 198, 198],\n",
       "        [188, 188, 188]],\n",
       "\n",
       "       [[129, 131, 131],\n",
       "        [130, 132, 132],\n",
       "        [131, 133, 133],\n",
       "        ...,\n",
       "        [203, 203, 203],\n",
       "        [210, 210, 210],\n",
       "        [201, 201, 201]],\n",
       "\n",
       "       [[132, 134, 134],\n",
       "        [132, 134, 134],\n",
       "        [133, 135, 135],\n",
       "        ...,\n",
       "        [194, 194, 194],\n",
       "        [198, 198, 198],\n",
       "        [190, 190, 190]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  6,   6,   6],\n",
       "        [  6,   6,   6],\n",
       "        [  6,   6,   6],\n",
       "        ...,\n",
       "        [124, 126, 127],\n",
       "        [144, 146, 147],\n",
       "        [130, 132, 133]],\n",
       "\n",
       "       [[  6,   6,   6],\n",
       "        [  6,   6,   6],\n",
       "        [  6,   6,   6],\n",
       "        ...,\n",
       "        [126, 128, 129],\n",
       "        [139, 141, 142],\n",
       "        [143, 145, 146]],\n",
       "\n",
       "       [[  6,   6,   6],\n",
       "        [  6,   6,   6],\n",
       "        [  6,   6,   6],\n",
       "        ...,\n",
       "        [158, 160, 161],\n",
       "        [123, 125, 126],\n",
       "        [146, 148, 149]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_path = os.path.join(r\"E:\\Random Python Scripts\\Tata HaxS\\Car dentss.v1i.coco-segmentation\\train\\001_jpg.rf.2cb35a9ef4733d3a87e2c2a2daee19a0.jpg\")\n",
    "sample_img = cv2.imread(sample_path)\n",
    "\n",
    "sample_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37T0xYI4Ke9d"
   },
   "source": [
    "## 01 Segment image with SAM\n",
    "Now we'll quickly use the wrapper classes from SAM, to segment our images, and we'll see why we need to fine-tune the model directly later. If you have not done so, you need to download the sam model see [here](https://github.com/facebookresearch/segment-anything/tree/main#model-checkpoints).\n",
    "\n",
    "Segment anything provides a class \"SamAutomaticMaskGenerator\". This class generates all masks for the whole image. However, it returns each mask separately, that's why we need a helper function to combine all masks in one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HJBVqzUux7tL"
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "def build_totalmask(pred: List[Dict[str, Any]]) -> np.ndarray:\n",
    "    \"\"\"Builds a total mask from a list of segmentations\n",
    "    ARGS:\n",
    "        pred (list): list of dicts with keys 'segmentation' and others\n",
    "    RETURNS:\n",
    "        total_mask (np.ndarray): total mask\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    total_mask = np.zeros(pred[0]['segmentation'].shape, dtype=np.uint8)\n",
    "    for seg in pred:\n",
    "        total_mask += seg['segmentation']\n",
    "    # use cv2 to make image black and white\n",
    "    _, total_mask = cv2.threshold(total_mask, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "\n",
    "    return total_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HD_gWqamK_AA",
    "outputId": "bf5aff51-6d08-452d-ddbb-429ddf46923c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\random python scripts\\tata haxs\\sam\\segment-anything\\segment_anything\\build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=r\"E:\\Random Python Scripts\\Tata HaxS\\SAM\\sam_vit_h_4b8939.pth\") #TODO your path here\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WgGerzhxtgac"
   },
   "outputs": [],
   "source": [
    "sam = sam.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9VNAnEP_KK5",
    "outputId": "b22523ca-d4a1-4af5-aff9-f4d87ec9af17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# generate masks and investigate object\n",
    "# hint: run this with GPU support\n",
    "masks = mask_generator.generate(sample_img)\n",
    "print(type(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YxQNW4P_gkm",
    "outputId": "8c80fe94-4efd-4896-ddda-06f8d5fc670c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "<class 'dict'>\n",
      "keys of dict: dict_keys(['segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'])\n"
     ]
    }
   ],
   "source": [
    "print(len(masks))\n",
    "print(type(masks[0]))\n",
    "print(f'keys of dict: {masks[0].keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XT7PeRQSBYdC",
    "outputId": "da35c128-45a2-440a-d3a9-d195759662ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of masks: (640, 640)\n",
      "Value counts in segmentation of first mask:\n",
      "(array([False,  True]), array([375545,  34055], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of masks: {masks[0]['segmentation'].shape}\")\n",
    "print('Value counts in segmentation of first mask:')\n",
    "print(np.unique(masks[0]['segmentation'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7s14BEuiB_tD"
   },
   "outputs": [],
   "source": [
    "total_mask = build_totalmask(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwOBqml4v_l6"
   },
   "source": [
    "When you use the web app of SAM, you might notice, that you need to provide a prompt (i.e. point with your mouse where your object is) to get a result. The mask_generator does this for you, by providing a grid of points over the whole image and creating a mask for each point and then later removing duplicated and low-quality masks. See the point grid below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "id": "RShjKGz0vva4",
    "outputId": "111562ad-ff70-4c44-83f5-ef7eb29038ba"
   },
   "outputs": [],
   "source": [
    "points = mask_generator.point_grids[0]\n",
    "# plot image and lay points on it\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(sample_img)\n",
    "ax.scatter(x=points[:, 0] *512, y=points[:, 1] *512, c=\"r\", s=10)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22HFyVV-_jFT"
   },
   "source": [
    "### Summary Segment Anything out of the box:\n",
    "- SAM takes any image and preprocess it for you\n",
    "- The SamAutomaticMaskGenerator class infers all masks of the image for you.\n",
    "- The output of the mask generator is a list of dictionaries, where each list item represents a mask.\n",
    "- The mask generator uses a grid of points as prompts and generates masks for each point.[see here](https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py)\n",
    "- The SamAutomaticMaskGenerator cannot be fine-tuned, because Facebook coded it with no_grad, and the output gets postprocessed.\n",
    "- So we will need the sam model directly to fine-tune. Therefore we'll define a few helper classes in the next section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seKWpfOqE9-E"
   },
   "source": [
    "## 02 Define models and classes for fine-tuning\n",
    "To make our life more easy, we'll define some functions and classes here:\n",
    "- Class COCODataset: helper to read in the annotations.json in coco format\n",
    "- class ResizeAndPad: helper to preprocess images to fit the expected size from SAM\n",
    "- load_datasets: function to get annotated COCO images into PyTorch dataloader\n",
    "\n",
    "You don't need to change anything here, you can just use the classes and functions later on.\n",
    "Feel free to skim through the code to gain better comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAeC2TPHbCQU"
   },
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to load data from a json file in COCO format.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    root_dir : str\n",
    "        the root directory containing the images and annotations\n",
    "    annotation_file : str\n",
    "        name of the json file containing the annotations (in root_dir)\n",
    "    transform : callable\n",
    "        a function/transform to apply to each image\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(idx)\n",
    "        returns the image, image path, and masks for the given index\n",
    "    buid_total_mask(masks)\n",
    "        combines the masks into a single mask\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "        # Filter out image_ids without any annotations\n",
    "        self.image_ids = [image_id for image_id in self.image_ids if len(self.coco.getAnnIds(imgIds=image_id)) > 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.root_dir, image_info['file_name'])\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        bboxes = []\n",
    "        masks = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            bboxes.append([x, y, x + w, y + h])\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image, masks, bboxes = self.transform(image, masks, np.array(bboxes))\n",
    "\n",
    "        bboxes = np.stack(bboxes, axis=0)\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        return image, image_path, torch.tensor(masks).float()\n",
    "\n",
    "    def get_totalmask(self, masks):\n",
    "        \"\"\"get all masks in to one image\n",
    "        ARGS:\n",
    "            masks (List[Tensor]): list of masks\n",
    "        RETURNS:\n",
    "            total_gt (Tensor): all masks in one image\n",
    "\n",
    "        \"\"\"\n",
    "        total_gt = torch.zeros_like(masks[0][0,:,:])\n",
    "        for k in range(len(masks[0])):\n",
    "            total_gt += masks[0][k,:,:]\n",
    "        return total_gt\n",
    "\n",
    "\n",
    "\n",
    "class ResizeAndPad:\n",
    "    \"\"\"\n",
    "    Resize and pad images and masks to a target size.\n",
    "\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_size : int\n",
    "        the target size of the image\n",
    "    transform : ResizeLongestSide\n",
    "        a transform to resize the image and masks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "        self.transform = ResizeLongestSide(target_size)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, image, masks, bboxes):\n",
    "        # Resize image and masks\n",
    "        og_h, og_w, _ = image.shape\n",
    "        image = self.transform.apply_image(image)\n",
    "        masks = [torch.tensor(self.transform.apply_image(mask)) for mask in masks]\n",
    "        image = self.to_tensor(image)\n",
    "\n",
    "        # Pad image and masks to form a square\n",
    "        _, h, w = image.shape\n",
    "        max_dim = max(w, h)\n",
    "        pad_w = (max_dim - w) // 2\n",
    "        pad_h = (max_dim - h) // 2\n",
    "\n",
    "        padding = (pad_w, pad_h, max_dim - w - pad_w, max_dim - h - pad_h)\n",
    "        image = transforms.Pad(padding)(image)\n",
    "        masks = [transforms.Pad(padding)(mask) for mask in masks]\n",
    "\n",
    "        # Adjust bounding boxes\n",
    "        bboxes = self.transform.apply_boxes(bboxes, (og_h, og_w))\n",
    "        bboxes = [[bbox[0] + pad_w, bbox[1] + pad_h, bbox[2] + pad_w, bbox[3] + pad_h] for bbox in bboxes]\n",
    "\n",
    "        return image, masks, bboxes\n",
    "\n",
    "\n",
    "def load_datasets(img_size):\n",
    "    \"\"\" load the training and validation datasets in PyTorch DataLoader objects\n",
    "    ARGS:\n",
    "        img_size (Tuple(int, int)): image size\n",
    "    RETURNS:\n",
    "        train_dataloader (DataLoader): training dataset\n",
    "        val_dataloader (DataLoader): validation dataset\n",
    "\n",
    "    \"\"\"\n",
    "    transform = ResizeAndPad(1024)\n",
    "    traindata = COCODataset(root_dir=TRAIN,\n",
    "                        annotation_file=os.path.join(TRAIN, ANNOT),\n",
    "                        transform=transform)\n",
    "    valdata = COCODataset(root_dir=TEST,\n",
    "                      annotation_file=os.path.join(TEST, ANNOT),\n",
    "                      transform=transform)\n",
    "    train_dataloader = DataLoader(traindata,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=1)\n",
    "    val_dataloader = DataLoader(valdata,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                num_workers=1)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8syP6e2bCQW",
    "outputId": "c4c9d18e-7538-4c3c-ee92-042443f606da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.66s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "trainloader, validloader = load_datasets(1024)\n",
    "sample_img = trainloader.dataset[0][0]\n",
    "sample_mask = trainloader.dataset[0][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwxmF_hpbCQX",
    "outputId": "28f0d500-1075-4a1e-c1df-5cc8f095af75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sample_img: torch.Size([3, 1024, 1024])\n",
      "shape fo sample_mask: torch.Size([4, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of sample_img: {sample_img.shape}')\n",
    "print(f'shape fo sample_mask: {sample_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i1znVjo0KQg"
   },
   "source": [
    "### Summary functions and classes:\n",
    "Now we have transformed our images and coco annotations to torch tensors, that we can use for training. For training (fine-tuning SAM) we need to define a Neural net with PyTorch first, we do this in the next class. It's pretty well documented, so I'll leave you with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "46NcQC-A3jfy"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ModelSimple(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for the sam model to to fine-tune the model on a new dataset\n",
    "\n",
    "    ...\n",
    "    Attributes:\n",
    "    -----------\n",
    "    freeze_encoder (bool): freeze the encoder weights\n",
    "    freeze_decoder (bool): freeze the decoder weights\n",
    "    freeze_prompt_encoder (bool): freeze the prompt encoder weights\n",
    "    transform (ResizeLongestSide): resize the images to the model input size\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    setup(): load the model and freeze the weights\n",
    "    forward(images, points): forward pass of the model, returns the masks and iou_predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, freeze_encoder=True, freeze_decoder=False, freeze_prompt_encoder=True):\n",
    "        super().__init__()\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.freeze_decoder = freeze_decoder\n",
    "        self.freeze_prompt_encoder = freeze_prompt_encoder\n",
    "        # we need this to make the input image size compatible with the model\n",
    "        self.transform = ResizeLongestSide(1024) #This is 1024, because sam was trained on 1024x1024 images\n",
    "\n",
    "    def setup(self):\n",
    "        # TODO your path here\n",
    "        self.model = sam_model_registry[\"vit_h\"](checkpoint=\"/content/sam_vit_h_4b8939.pth\")\n",
    "        #self.model = sam_model_registry['vit_h'](os.path.join('/content/drive/MyDrive/ColabNotebooks/finetunesam','sam_vit_h_4b8939.pth'))\n",
    "        # to speed up training time, we normally freeze the encoder and decoder\n",
    "        if self.freeze_encoder:\n",
    "            for param in self.model.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_prompt_encoder:\n",
    "            for param in self.model.prompt_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if self.freeze_decoder:\n",
    "            for param in self.model.mask_decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.transfrom = ResizeLongestSide(self.model.image_encoder.img_size)\n",
    "    def forward(self, images):\n",
    "        _, _, H, W = images.shape # batch, channel, height, width\n",
    "        image_embeddings = self.model.image_encoder(images) # shape: (1, 256, 64, 64)\n",
    "        # get prompt embeddings without acutally any prompts (uninformative)\n",
    "        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "\n",
    "        # get low resolution masks and iou predictions\n",
    "        # mulitmask_output=False means that we only get one mask per image,\n",
    "        # otherwise we would get three masks per image\n",
    "        low_res_masks, iou_predictions = self.model.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=self.model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # sparse_embeddings shape: (1, 0, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # dense_embeddings shape: (1, 256, 256)\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        # postprocess the masks to get the final masks and resize them to the original image size\n",
    "        masks = F.interpolate(\n",
    "            low_res_masks, # shape: (1, 1, 256, 256)\n",
    "            (H, W),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        # shape masks after interpolate: torch.Size([1, 1, 1024, 1024])\n",
    "        return masks, iou_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2fwU8qI4oKH",
    "outputId": "1026842e-b60c-46ef-b78c-b44e7866ee8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "model = ModelSimple()\n",
    "model.setup()\n",
    "img_size = model.model.image_encoder.img_size\n",
    "print(img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxu5a8633RQ-"
   },
   "source": [
    "## 03 Plan for fine-tuning\n",
    "When you've read the code in the cell above, this should be clear. We'll use the SAM model directly to output a 1024,1024 mask. This will be a probability map of the location of masks. As you see in line 45 above we don't provide any prompts. We do this to save us work with the postprocessing. To get all our masks for the image, we'll just put our ground truth masks (all masks for one image) in the loss function. So that we'll force the model to output high values for areas with masks.\n",
    "\n",
    "Now alt that's left is defining the loss functions and the training loop. For the loss function, we use the same as the authors of the SAM paper:\n",
    "\n",
    "Quote from Sam Paper\n",
    "```\n",
    "Losses. We supervise mask prediction with a linear combination of focal loss [65] and dice loss [73] in a 20:1 ratio of\n",
    "focal loss to dice loss, following [20, 14]. Unlike [20, 14],\n",
    "we observe that auxiliary deep supervision after each decoder layer is unhelpful. The IoU prediction head is trained\n",
    "with mean-square-error loss between the IoU prediction and\n",
    "the predicted mask's IoU with the ground truth mask. It is\n",
    "added to the mask loss with a constant scaling factor of 1.0.\n",
    "```\n",
    "We won't train the IoU prediction head in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPhQN81W3QyN"
   },
   "source": [
    "## 04 Models, classes functions for fine-tuning\n",
    "To make our code more clear, we'll define the following classes and functions to fine-tune SAM:\n",
    "- get_totalmask (function): combines all single object masks into one mask per image\n",
    "- FocalLoss (class): to calculate Focalloss\n",
    "- DiceLoss (class): to calculate Diceloss\n",
    "- criterion (function): combines Dice- and Focalloss. This is our final loss function\n",
    "- train_one_epoch(function): calculates loss for each batch\n",
    "- train (function) calls train_one_epoch for each epoch and calculates validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Rji2T7bCQa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "5GhzOeOFbCQa"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def get_totalmask(masks):\n",
    "    \"\"\"get all masks in to one image\n",
    "    ARGS:\n",
    "        masks (torch.Tensor): shape: (N, H, W) where N is the number of masks\n",
    "                              masks H,W is usually 1024,1024\n",
    "    RETURNS:\n",
    "        total_gt (torch.Tensor): all masks in one image\n",
    "\n",
    "    \"\"\"\n",
    "    total_gt = torch.zeros_like(masks[0,:,:])\n",
    "    for k in range(len(masks)):\n",
    "        total_gt += masks[k,:,:]\n",
    "    return total_gt\n",
    "\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Computes the Focal loss. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = ALPHA * (1 - BCE_EXP)**GAMMA * BCE\n",
    "\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\" Computes the Dice loss. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = F.sigmoid(inputs)\n",
    "        inputs = inputs.flatten(0,2)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + smooth) / \\\n",
    "            (inputs.sum() + targets.sum() + smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "\n",
    "def criterion(x, y):\n",
    "    \"\"\" Combined dice and focal loss.\n",
    "    ARGS:\n",
    "        x: (torch.Tensor) the model output\n",
    "        y: (torch.Tensor) the target\n",
    "    RETURNS:\n",
    "        (torch.Tensor) the combined loss\n",
    "\n",
    "    \"\"\"\n",
    "    focal, dice = FocalLoss(), DiceLoss()\n",
    "    y = y.to(DEVICE)\n",
    "    x = x.to(DEVICE)\n",
    "    return 20 * focal(x, y) + dice(x, y)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, trainloader, optimizer, epoch_idx, tb_writer):\n",
    "    \"\"\" Runs forward and backward pass for one epoch and returns the average\n",
    "    batch loss for the epoch.\n",
    "    ARGS:\n",
    "        model: (nn.Module) the model to train\n",
    "        trainloader: (torch.utils.data.DataLoader) the dataloader for training\n",
    "        optimizer: (torch.optim.Optimizer) the optimizer to use for training\n",
    "        epoch_idx: (int) the index of the current epoch\n",
    "        tb_writer: (torch.utils.tensorboard.writer.SummaryWriter) the tensorboard writer\n",
    "    RETURNS:\n",
    "        last_loss: (float) the average batch loss for the epoch\n",
    "\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    for i, (image, path, masks) in enumerate(trainloader):\n",
    "\n",
    "        image = image.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model(image)\n",
    "        masks = masks[0].to(DEVICE)\n",
    "        total_mask = get_totalmask(masks)\n",
    "        pred = pred.to(DEVICE)\n",
    "        loss = criterion(pred, total_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    i = len(trainloader)\n",
    "    last_loss = running_loss / i\n",
    "    print(f'batch_loss for batch {i}: {last_loss}')\n",
    "    tb_x = epoch_idx * len(trainloader) + i + 1\n",
    "    tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "    running_loss = 0.\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def train_fn():\n",
    "    \"\"\" Trains the model for the given number of EPOCHS.\"\"\"\n",
    "    bestmodel_path = \"\"\n",
    "    model = ModelSimple()\n",
    "    model.setup()\n",
    "    model.to(DEVICE)\n",
    "    img_size = model.model.image_encoder.img_size\n",
    "    trainloader, validloader = load_datasets(img_size=img_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    best_valid_loss = float('inf')\n",
    "    timestamp_writer = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    writer = SummaryWriter(os.path.join(\"/content/drive/MyDrive/SAM\", f\"trainer_{timestamp_writer}\")) # TODO your path here\n",
    "    for epch in range(EPOCHS): # type: ignore\n",
    "        running_vloss = 0.\n",
    "        model.train(True)\n",
    "        avg_batchloss = train_one_epoch(\n",
    "            model, trainloader, optimizer, epch, writer)\n",
    "        if not EVALUATE: # type: ignore\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            for images, path, masks in validloader:\n",
    "                model.to(DEVICE)\n",
    "                images = images.to(DEVICE)\n",
    "                masks = masks[0].to(DEVICE)\n",
    "                total_mask = get_totalmask(masks)\n",
    "                total_mask = total_mask.to(DEVICE)\n",
    "                model.eval()\n",
    "                preds, iou = model(images)\n",
    "                preds = preds.to(DEVICE)\n",
    "                vloss = criterion(preds, total_mask)\n",
    "                running_vloss += vloss.item()\n",
    "        print(f'epoch: {epch}, validloss: {running_vloss}')\n",
    "        avg_vloss = running_vloss / len(validloader)\n",
    "        # save model\n",
    "        print(f'epoch: {epch}, validloss: {running_vloss}')\n",
    "        print(f'best valid loss: {best_valid_loss}')\n",
    "        if running_vloss < best_valid_loss:\n",
    "          best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFWOAdEY7upr"
   },
   "source": [
    "## 05 Wrap up and next steps\n",
    "\n",
    "Training the model on Colab might run out of RAM, when you don't have premium (at least it did for me) I've trained the model on an HPC cluster. I'll leave you with the code to train the model and also some functions to visualize and evaluate.\n",
    "\n",
    "### Train:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSqrkj4w7uLh",
    "outputId": "bf5833d7-aef1-493d-fa88-7818c1a1d6b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    }
   ],
   "source": [
    "model = train_fn()\n",
    "#torch.save(model.state_dict(), os.path.join('/content/drive/MyDrive/ColabNotebooks/finetunesam','finetunet.pth')) # TODO your path here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FrAGnJX8vwH"
   },
   "source": [
    "## Predict trained model\n",
    "\n",
    "Here I assume that you have fine-tuned SAM with the code above and the fine-tuned model is saved as model_final.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf8QC8hobCQe"
   },
   "outputs": [],
   "source": [
    "# use my custom SAM model wrapper class\n",
    "#model_trained= ModelSimple()\n",
    "#model_trained.setup()\n",
    "#model_trained.load_state_dict(torch.load('finetuned.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU1aS-ow9ZUy"
   },
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    " #   model_trained.load_state_dict(torch.load('finetuned.pth', map_location=torch.device('cpu')))\n",
    " #   model_trained.eval()\n",
    "  #  img_size = model_trained.model.image_encoder.img_size\n",
    "   # print(f'img_size: {img_size}')\n",
    "    #print(f'img_size: {model_trained.model.image_encoder.img_size}')\n",
    "    # TODO your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwTOPNiemlQ7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "sam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
