{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9f215-c8f0-4069-8217-62ee243a5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from patchify import patchify  # Only to handle large images\n",
    "from PIL import Image  # Use Pillow for image handling\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d86347-2590-4463-9f88-f4c14763e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Hyperparameters ------------------ #\n",
    "image_dir = r\"E:/Random Python Scripts/Tata HaxS/SAM/Dataset/train/images\"  # Image directory path\n",
    "mask_dir = r\"E:/Random Python Scripts/Tata HaxS/SAM/Dataset/train/masks\"    # Mask directory path\n",
    "\n",
    "patch_size = 256          # Size of image patches\n",
    "step = 256                # Step size for patchify\n",
    "batch_size = 2            # Batch size for DataLoader\n",
    "learning_rate = 1e-5      # Learning rate for optimizer\n",
    "weight_decay = 0          # Weight decay for optimizer\n",
    "num_epochs = 10           # Number of epochs for training\n",
    "loss_fn_type = 'DiceCELoss'  # Loss function: Choose between DiceFocalLoss, FocalLoss, DiceCELoss\n",
    "model_save_path = r\"E:/Random Python Scripts/Tata HaxS/Models/Models/SAM/Lmao/lmao2.pth\"  # Path to save model\n",
    "\n",
    "# ------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0de003-c803-4645-947a-cf1a89fa86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "mask_files = [f for f in os.listdir(mask_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b151b0e-022e-4eed-a722-9093035f7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_patches = []\n",
    "all_mask_patches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa876274-132a-47d9-8f4f-73fdb3dd22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images\n",
    "for img_file in image_files:\n",
    "    # Load each image using Pillow\n",
    "    try:\n",
    "        # Convert grayscale image to RGB\n",
    "        large_image = Image.open(os.path.join(image_dir, img_file)).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {img_file} - {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Convert the Pillow image to a NumPy array for patching\n",
    "    large_image_np = np.array(large_image)\n",
    "    \n",
    "    # Patchify the large image (with 3 channels for RGB)\n",
    "    patches_img = patchify(large_image_np, (patch_size, patch_size, 3), step=step)\n",
    "\n",
    "    for i in range(patches_img.shape[0]):\n",
    "        for j in range(patches_img.shape[1]):\n",
    "            single_patch_img = patches_img[i, j, :, :, :]\n",
    "            all_img_patches.append(single_patch_img)\n",
    "\n",
    "images = np.array(all_img_patches)\n",
    "images = np.array(all_img_patches).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49733ed-5635-4af2-9c84-0b6a806a98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process masks\n",
    "for mask_file in mask_files:\n",
    "    # Load each mask using Pillow\n",
    "    try:\n",
    "        large_mask = Image.open(os.path.join(mask_dir, mask_file)).convert('L')  # Load as grayscale\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading mask: {mask_file} - {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Convert the Pillow mask to a NumPy array for patching\n",
    "    large_mask_np = np.array(large_mask)\n",
    "\n",
    "    # Patchify the large mask\n",
    "    patches_mask = patchify(large_mask_np, (patch_size, patch_size), step=step)\n",
    "\n",
    "    for i in range(patches_mask.shape[0]):\n",
    "        for j in range(patches_mask.shape[1]):\n",
    "            single_patch_mask = patches_mask[i, j, :, :]\n",
    "            single_patch_mask = (single_patch_mask / 255.).astype(np.uint8)  # Normalize mask values to [0, 1]\n",
    "            all_mask_patches.append(single_patch_mask)\n",
    "\n",
    "masks = np.array(all_mask_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4698b-0d8f-4c3b-95a1-769099c8c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the indices of non-empty masks\n",
    "valid_indices = [i for i, mask in enumerate(masks) if mask.max() != 0]\n",
    "# Filter the image and mask arrays to keep only the non-empty pairs\n",
    "filtered_images = images[valid_indices]\n",
    "filtered_masks = masks[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a81352-9435-4849-9953-3f180af8d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "dataset_dict = {\n",
    "    \"image\": [Image.fromarray(img) for img in filtered_images],\n",
    "    \"label\": [Image.fromarray(mask) for mask in filtered_masks],\n",
    "}\n",
    "\n",
    "# Create the dataset using the datasets.Dataset class\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cdffba-1581-48eb-8225-92d199e8e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "img_num = random.randint(0, filtered_images.shape[0] - 1)\n",
    "example_image = dataset[img_num][\"image\"]\n",
    "example_mask = dataset[img_num][\"label\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first image on the left\n",
    "axes[0].imshow(np.array(example_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "# Plot the second image on the right\n",
    "axes[1].imshow(example_mask, cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Mask\")\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Display the images side by side\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a24b7-cfa8-451b-81f9-5629f2366697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bounding boxes from mask\n",
    "def get_bounding_box(ground_truth_map):\n",
    "    # Get bounding box from mask\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    # Add perturbation to bounding box coordinates\n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "    bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce242b-9f99-4f5a-bf01-d1b29cc38e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for SAM model\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import SamProcessor\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"].convert('RGB')  # Ensure the image is RGB\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "\n",
    "        # Get bounding box prompt\n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "        # Prepare image and prompt for the model\n",
    "        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "        # Remove batch dimension which the processor adds by default\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        # Add ground truth segmentation\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b531f-97d1-47a1-a3b5-6bd4e65d0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor\n",
    "from transformers import SamProcessor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04b1b7-e312-46d4-868a-18b5e0e99aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the SAMDataset\n",
    "train_dataset = SAMDataset(dataset=dataset, processor=processor)\n",
    "\n",
    "example = train_dataset[0]\n",
    "for k,v in example.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08577742-a623-464e-9686-d45155f4cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader instance for the training dataset\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557ed3d-6f72-46a6-87ae-6af5a4adb63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from transformers import SamModel\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c19c1e-eecb-4529-a3f9-586fdf865cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "# Initialize the optimizer and the loss function\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Try DiceFocalLoss, FocalLoss, DiceCELoss based on the hyperparam\n",
    "if loss_fn_type == 'DiceCELoss':\n",
    "    seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "elif loss_fn_type == 'FocalLoss':\n",
    "    seg_loss = monai.losses.FocalLoss(sigmoid=True)\n",
    "elif loss_fn_type == 'DiceFocalLoss':\n",
    "    seg_loss = monai.losses.DiceFocalLoss(sigmoid=True, squared_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90043626-a6b6-421d-bc1d-779d75173f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "#Training loop\n",
    "model.to(device)\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "      # forward pass\n",
    "      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                      input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                      multimask_output=False)\n",
    "\n",
    "      # compute loss\n",
    "      predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "      # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "\n",
    "      # optimize\n",
    "      optimizer.step()\n",
    "      epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5ee47-7788-4fad-8f3e-da444ca02b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary to a file\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_n",
   "language": "python",
   "name": "sam_n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
