{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850a0dd-0765-4e5a-9785-e0fd2e8c84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import SamProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import SamModel\n",
    "from torch.optim import Adam\n",
    "import monai\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torchvision.transforms as transforms  # Import for resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa5142-6a57-4df4-8d85-5b00d58ed18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Hyperparameters ------------------ #\n",
    "image_dir = r\"E:/Random Python Scripts/Tata HaxS/SAM/Dataset/train/images\"  # Image directory path\n",
    "mask_dir = r\"E:/Random Python Scripts/Tata HaxS/SAM/Dataset/train/masks\"    # Mask directory path\n",
    "\n",
    "batch_size = 2            # Batch size for DataLoader\n",
    "learning_rate = 1e-5      # Learning rate for optimizer\n",
    "weight_decay = 0          # Weight decay for optimizer\n",
    "num_epochs = 10           # Number of epochs for training\n",
    "loss_fn_type = 'DiceCELoss'  # Loss function: Choose between DiceFocalLoss, FocalLoss, DiceCELoss\n",
    "model_save_path = r\"E:/Random Python Scripts/Tata HaxS/Models/Models/SAM/Lmao/lmao2.pth\"  # Path to save model\n",
    "\n",
    "# ------------------------------------------------------ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ac159-bc17-44b7-bdc4-08105f9240b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "mask_files = [f for f in os.listdir(mask_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "all_images = []\n",
    "all_masks = []\n",
    "\n",
    "# Transform for resizing\n",
    "resize_transform = transforms.Resize((256, 256))  # Adjust as necessary\n",
    "\n",
    "# Process images\n",
    "for img_file in image_files:\n",
    "    try:\n",
    "        large_image = Image.open(os.path.join(image_dir, img_file)).convert('RGB')\n",
    "        all_images.append(np.array(large_image))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {img_file} - {e}\")\n",
    "        continue\n",
    "\n",
    "# Process masks\n",
    "for mask_file in mask_files:\n",
    "    try:\n",
    "        large_mask = Image.open(os.path.join(mask_dir, mask_file)).convert('L')  # Load as grayscale\n",
    "        # Resize mask to match model's output shape\n",
    "        large_mask = resize_transform(large_mask)  # Resize the mask\n",
    "        all_masks.append(np.array(large_mask))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading mask: {mask_file} - {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "images = np.array(all_images)\n",
    "masks = np.array(all_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4424bb-0ed8-4db4-b8b1-5afe4a16d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the indices of non-empty masks\n",
    "valid_indices = [i for i, mask in enumerate(masks) if mask.max() != 0]\n",
    "filtered_images = images[valid_indices]\n",
    "filtered_masks = masks[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597fa18-2a0e-43ea-b16e-3534e72419fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "dataset_dict = {\n",
    "    \"image\": [Image.fromarray(img) for img in filtered_images],\n",
    "    \"label\": [Image.fromarray(mask) for mask in filtered_masks],\n",
    "}\n",
    "\n",
    "# Create the dataset using the datasets.Dataset class\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf75269-5dbc-43f6-82b4-8ab1f7f67d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "img_num = random.randint(0, filtered_images.shape[0] - 1)\n",
    "example_image = dataset[img_num][\"image\"]\n",
    "example_mask = dataset[img_num][\"label\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(np.array(example_image), cmap='gray')  # Assuming the first image is grayscale\n",
    "axes[0].set_title(\"Image\")\n",
    "axes[1].imshow(example_mask, cmap='gray')  # Assuming the second image is grayscale\n",
    "axes[1].set_title(\"Mask\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f75bb9-ce84-4b91-b442-bd314d42363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bounding boxes from mask\n",
    "def get_bounding_box(ground_truth_map):\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "    if len(x_indices) == 0 or len(y_indices) == 0:  # If no object found\n",
    "        return [0, 0, 0, 0]  # Return a dummy bounding box\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "    bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d93eb2-ac86-4b33-9771-655d5468d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for SAM model\n",
    "class SAMDataset(TorchDataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"].convert('RGB')\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "\n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edada724-86c8-48ca-b2b6-12bd51743264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "train_dataset = SAMDataset(dataset=dataset, processor=processor)\n",
    "\n",
    "example = train_dataset[0]\n",
    "for k,v in example.items():\n",
    "    print(k,v.shape)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc6e54-8c7c-41f1-863a-3219d03fc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea748e31-a850-430e-8766-1facb9f59a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure only the mask decoder gradients are computed\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "# Initialize optimizer and loss function based on hyperparameter choice\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "if loss_fn_type == 'DiceCELoss':\n",
    "    seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "elif loss_fn_type == 'FocalLoss':\n",
    "    seg_loss = monai.losses.FocalLoss(sigmoid=True)\n",
    "elif loss_fn_type == 'DiceFocalLoss':\n",
    "    seg_loss = monai.losses.DiceFocalLoss(sigmoid=True, squared_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d21372-d127-4eb2-a70e-80828f3851c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9583b1-0769-4f66-ad7e-a9a510254c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary to a file\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_n",
   "language": "python",
   "name": "sam_n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
