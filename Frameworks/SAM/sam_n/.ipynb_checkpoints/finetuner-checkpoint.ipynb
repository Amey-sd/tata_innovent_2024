{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbb1aa3-3f32-4a1c-a54b-23107e5012dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "from collections import defaultdict, deque\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from sahi.utils.coco import Coco\n",
    "from sahi.utils.cv import get_bool_mask_from_coco_segmentation\n",
    "\n",
    "import torch\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.distributed as dist\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from transformers.models.maskformer.modeling_maskformer import dice_loss, sigmoid_focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b503892-c3d8-44e3-bff9-b03c777d1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the SAM directory to the system path\n",
    "sys.path.append(\"./segment-anything\")\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "NUM_WORKERS = 0  # https://github.com/pytorch/pytorch/issues/42518\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7b3b86-7602-4286-9343-f555cd731dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/facebookresearch/detectron2/blob/main/detectron2/utils/comm.py\n",
    "def get_world_size():\n",
    "    if not dist.is_available():\n",
    "        return 1\n",
    "    if not dist.is_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e8bb7ee-7f7c-4998-97a4-6d8a2c36fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/facebookresearch/detectron2/blob/main/detectron2/utils/comm.py\n",
    "def all_gather(data):\n",
    "    \"\"\"\n",
    "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
    "    Args:\n",
    "        data: any picklable object\n",
    "    Returns:\n",
    "        list[data]: list of data gathered from each rank\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "    # serialized to a Tensor\n",
    "    buffer = pickle.dumps(data)\n",
    "    storage = torch.ByteStorage.from_buffer(buffer)\n",
    "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
    "\n",
    "    # obtain Tensor size of each rank\n",
    "    local_size = torch.LongTensor([tensor.numel()]).to(\"cuda\")\n",
    "    size_list = [torch.LongTensor([0]).to(\"cuda\") for _ in range(world_size)]\n",
    "    dist.all_gather(size_list, local_size)\n",
    "    size_list = [int(size.item()) for size in size_list]\n",
    "    max_size = max(size_list)\n",
    "\n",
    "    # receiving Tensor from all ranks\n",
    "    # we pad the tensor because torch all_gather does not support\n",
    "    # gathering tensors of different shapes\n",
    "    tensor_list = []\n",
    "    for _ in size_list:\n",
    "        tensor_list.append(torch.ByteTensor(size=(max_size,)).to(\"cuda\"))\n",
    "    if local_size != max_size:\n",
    "        padding = torch.ByteTensor(size=(max_size - local_size,)).to(\"cuda\")\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    dist.all_gather(tensor_list, tensor)\n",
    "\n",
    "    data_list = []\n",
    "    for size, tensor in zip(size_list, tensor_list):\n",
    "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
    "        data_list.append(pickle.loads(buffer))\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb1f876-62aa-4642-82d5-cfc7f1fa23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco mask style dataloader\n",
    "class Coco2MaskDataset(Dataset):\n",
    "    def __init__(self, data_root, split, image_size):\n",
    "        self.data_root = data_root\n",
    "        self.split = split\n",
    "        self.image_size = image_size\n",
    "        annotation = os.path.join(data_root, split, \"_annotations.coco.json\")\n",
    "        self.coco = Coco.from_coco_dict_or_path(annotation)\n",
    "\n",
    "        # TODO: use ResizeLongestSide and pad to square\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.image_resize = transforms.Resize((image_size, image_size), interpolation=Image.BILINEAR)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        coco_image = self.coco.images[index]\n",
    "        image = Image.open(os.path.join(self.data_root, self.split, coco_image.file_name)).convert(\"RGB\")\n",
    "        original_width, original_height = image.width, image.height\n",
    "        ratio_h = self.image_size / image.height\n",
    "        ratio_w = self.image_size / image.width\n",
    "        image = self.image_resize(image)\n",
    "        image = self.to_tensor(image)\n",
    "        image = self.normalize(image)\n",
    "\n",
    "        bboxes = []\n",
    "        masks = []\n",
    "        labels = []\n",
    "        for annotation in coco_image.annotations:\n",
    "            x, y, w, h = annotation.bbox\n",
    "            # get scaled bbox in xyxy format\n",
    "            bbox = [x * ratio_w, y * ratio_h, (x + w) * ratio_w, (y + h) * ratio_h]\n",
    "            mask = get_bool_mask_from_coco_segmentation(annotation.segmentation, original_width, original_height)\n",
    "            mask = cv2.resize(mask, (self.image_size, self.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "            mask = (mask > 0.5).astype(np.uint8)\n",
    "            label = annotation.category_id\n",
    "            bboxes.append(bbox)\n",
    "            masks.append(mask)\n",
    "            labels.append(label)\n",
    "        bboxes = np.stack(bboxes, axis=0)\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        labels = np.stack(labels, axis=0)\n",
    "        return image, torch.tensor(bboxes), torch.tensor(masks).long()\n",
    "    \n",
    "    @classmethod\n",
    "    def collate_fn(cls, batch):\n",
    "        images, bboxes, masks = zip(*batch)\n",
    "        images = torch.stack(images, dim=0)\n",
    "        return images, bboxes, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce32d95-ea60-44c0-84e1-1233cba61a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMFinetuner(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_type,\n",
    "            checkpoint_path,\n",
    "            freeze_image_encoder=False,\n",
    "            freeze_prompt_encoder=False,\n",
    "            freeze_mask_decoder=False,\n",
    "            batch_size=1,\n",
    "            learning_rate=1e-4,\n",
    "            weight_decay=1e-4,\n",
    "            train_dataset=None,\n",
    "            val_dataset=None,\n",
    "            metrics_interval=10,\n",
    "        ):\n",
    "        super(SAMFinetuner, self).__init__()\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.model = sam_model_registry[self.model_type](checkpoint=checkpoint_path)\n",
    "        self.model.to(device=self.device)\n",
    "        self.freeze_image_encoder = freeze_image_encoder\n",
    "        if freeze_image_encoder:\n",
    "            for param in self.model.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if freeze_prompt_encoder:\n",
    "            for param in self.model.prompt_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if freeze_mask_decoder:\n",
    "            for param in self.model.mask_decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "        self.train_metric = defaultdict(lambda: deque(maxlen=metrics_interval))\n",
    "\n",
    "        self.metrics_interval = metrics_interval\n",
    "\n",
    "    def forward(self, imgs, bboxes, labels):\n",
    "        _, _, H, W = imgs.shape\n",
    "        features = self.model.image_encoder(imgs)\n",
    "        num_masks = sum([len(b) for b in bboxes])\n",
    "\n",
    "        loss_focal = loss_dice = loss_iou = 0.\n",
    "        predictions = []\n",
    "        tp, fp, fn, tn = [], [], [], []\n",
    "        for feature, bbox, label in zip(features, bboxes, labels):\n",
    "            # Embed prompts\n",
    "            sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "                points=None,\n",
    "                boxes=bbox,\n",
    "                masks=None,\n",
    "            )\n",
    "            # Predict masks\n",
    "            low_res_masks, iou_predictions = self.model.mask_decoder(\n",
    "                image_embeddings=feature.unsqueeze(0),\n",
    "                image_pe=self.model.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "            # Upscale the masks to the original image resolution\n",
    "            masks = F.interpolate(\n",
    "                low_res_masks,\n",
    "                (H, W),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            predictions.append(masks)\n",
    "            # Compute the iou between the predicted masks and the ground truth masks\n",
    "            batch_tp, batch_fp, batch_fn, batch_tn = smp.metrics.get_stats(\n",
    "                masks,\n",
    "                label.unsqueeze(1),\n",
    "                mode='binary',\n",
    "                threshold=0.5,\n",
    "            )\n",
    "            batch_iou = smp.metrics.iou_score(batch_tp, batch_fp, batch_fn, batch_tn)\n",
    "            # Compute the loss\n",
    "            masks = masks.squeeze(1).flatten(1)\n",
    "            label = label.flatten(1)\n",
    "            loss_focal += sigmoid_focal_loss(masks, label.float(), num_masks)\n",
    "            loss_dice += dice_loss(masks, label.float(), num_masks)\n",
    "            loss_iou += F.mse_loss(iou_predictions, batch_iou, reduction='sum') / num_masks\n",
    "            tp.append(batch_tp)\n",
    "            fp.append(batch_fp)\n",
    "            fn.append(batch_fn)\n",
    "            tn.append(batch_tn)\n",
    "        return {\n",
    "            'loss': 20. * loss_focal + loss_dice + loss_iou,  # SAM default loss\n",
    "            'loss_focal': loss_focal,\n",
    "            'loss_dice': loss_dice,\n",
    "            'loss_iou': loss_iou,\n",
    "            'predictions': predictions,\n",
    "            'tp': torch.cat(tp),\n",
    "            'fp': torch.cat(fp),\n",
    "            'fn': torch.cat(fn),\n",
    "            'tn': torch.cat(tn),\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        imgs, bboxes, labels = batch\n",
    "        outputs = self(imgs, bboxes, labels)\n",
    "\n",
    "        for metric in ['tp', 'fp', 'fn', 'tn']:\n",
    "            self.train_metric[metric].append(outputs[metric])\n",
    "\n",
    "        # aggregate step metics\n",
    "        step_metrics = [torch.cat(list(self.train_metric[metric])) for metric in ['tp', 'fp', 'fn', 'tn']]\n",
    "        per_mask_iou = smp.metrics.iou_score(*step_metrics, reduction=\"micro-imagewise\")\n",
    "        metrics = {\n",
    "            \"loss\": outputs[\"loss\"],\n",
    "            \"loss_focal\": outputs[\"loss_focal\"],\n",
    "            \"loss_dice\": outputs[\"loss_dice\"],\n",
    "            \"loss_iou\": outputs[\"loss_iou\"],\n",
    "            \"train_per_mask_iou\": per_mask_iou,\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True, rank_zero_only=True)\n",
    "        return metrics\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        imgs, bboxes, labels = batch\n",
    "        outputs = self(imgs, bboxes, labels)\n",
    "        outputs.pop(\"predictions\")\n",
    "        return outputs\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if NUM_GPUS > 1:\n",
    "            outputs = all_gather(outputs)\n",
    "            # the outputs are a list of lists, so flatten it\n",
    "            outputs = [item for sublist in outputs for item in sublist]\n",
    "        # aggregate step metics\n",
    "        step_metrics = [\n",
    "            torch.cat(list([x[metric].to(self.device) for x in outputs]))\n",
    "            for metric in ['tp', 'fp', 'fn', 'tn']]\n",
    "        # per mask IoU means that we first calculate IoU score for each mask\n",
    "        # and then compute mean over these scores\n",
    "        per_mask_iou = smp.metrics.iou_score(*step_metrics, reduction=\"micro-imagewise\")\n",
    "\n",
    "        metrics = {\"val_per_mask_iou\": per_mask_iou}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        def warmup_step_lr_builder(warmup_steps, milestones, gamma):\n",
    "            def warmup_step_lr(steps):\n",
    "                if steps < warmup_steps:\n",
    "                    lr_scale = (steps + 1.) / float(warmup_steps)\n",
    "                else:\n",
    "                    lr_scale = 1.\n",
    "                    for milestone in sorted(milestones):\n",
    "                        if steps >= milestone * self.trainer.estimated_stepping_batches:\n",
    "                            lr_scale *= gamma\n",
    "                return lr_scale\n",
    "            return warmup_step_lr\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            opt,\n",
    "            warmup_step_lr_builder(250, [0.66667, 0.86666], 0.1)\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': opt,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': \"step\",\n",
    "                'frequency': 1,\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            collate_fn=self.train_dataset.collate_fn,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            shuffle=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            collate_fn=self.val_dataset.collate_fn,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            shuffle=False)\n",
    "        return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5567a151-b4dd-43ea-86e0-340481e5ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Directly set paths and hyperparameters here\n",
    "    data_root = r\"E:\\Random Python Scripts\\Tata HaxS\\sam_n\\sam.v2i.coco\"\n",
    "    model_type = \"vit_b\"  # or 'vit_h', 'vit_l'\n",
    "    checkpoint_path = \"/SAM/sam_vit_h_4b8939.pth\"\n",
    "    freeze_image_encoder = False\n",
    "    freeze_prompt_encoder = False\n",
    "    freeze_mask_decoder = False\n",
    "    batch_size = 2\n",
    "    image_size = 1024\n",
    "    steps = 200\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-2\n",
    "    metrics_interval = 50\n",
    "    output_dir = \"./output\"  # Path to save the model\n",
    "\n",
    "    # Load the dataset\n",
    "    train_dataset = Coco2MaskDataset(data_root=data_root, split=\"train\", image_size=image_size)\n",
    "    val_dataset = Coco2MaskDataset(data_root=data_root, split=\"val\", image_size=image_size)\n",
    "\n",
    "    # Create the model\n",
    "    model = SAMFinetuner(\n",
    "        model_type,\n",
    "        checkpoint_path,\n",
    "        freeze_image_encoder=freeze_image_encoder,\n",
    "        freeze_prompt_encoder=freeze_prompt_encoder,\n",
    "        freeze_mask_decoder=freeze_mask_decoder,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        metrics_interval=metrics_interval,\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=output_dir,\n",
    "            filename='{step}-{val_per_mask_iou:.2f}',\n",
    "            save_last=True,\n",
    "            save_top_k=1,\n",
    "            monitor=\"val_per_mask_iou\",\n",
    "            mode=\"max\",\n",
    "            save_weights_only=True,\n",
    "            every_n_train_steps=metrics_interval,\n",
    "        ),\n",
    "    ]\n",
    "    trainer = pl.Trainer(\n",
    "        strategy='ddp' if NUM_GPUS > 1 else None,\n",
    "        accelerator=DEVICE,\n",
    "        devices=NUM_GPUS,\n",
    "        precision=32,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=-1,\n",
    "        max_steps=steps,\n",
    "        val_check_interval=metrics_interval,\n",
    "        check_val_every_n_epoch=None,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33c73420-796e-40d5-9220-1862bda83e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations:  71%|████████████████████████████████████▊               | 268/378 [00:00<00:00, 3225.89it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A linearring requires at least 4 coordinates.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path to save the model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCoco2MaskDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m Coco2MaskDataset(data_root\u001b[38;5;241m=\u001b[39mdata_root, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_size\u001b[38;5;241m=\u001b[39mimage_size)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create the model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mCoco2MaskDataset.__init__\u001b[1;34m(self, data_root, split, image_size)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m image_size\n\u001b[0;32m      7\u001b[0m annotation \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_root, split, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_annotations.coco.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco \u001b[38;5;241m=\u001b[39m \u001b[43mCoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_coco_dict_or_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# TODO: use ResizeLongestSide and pad to square\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_n\\lib\\site-packages\\sahi\\utils\\coco.py:1152\u001b[0m, in \u001b[0;36mCoco.from_coco_dict_or_path\u001b[1;34m(cls, coco_dict_or_path, image_dir, remapping_dict, ignore_negative_samples, clip_bboxes_to_img_dims, use_threads, num_threads)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;66;03m# get category name (id:name)\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     category_name \u001b[38;5;241m=\u001b[39m category_mapping[category_id]\n\u001b[1;32m-> 1152\u001b[0m     coco_annotation \u001b[38;5;241m=\u001b[39m \u001b[43mCocoAnnotation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_coco_annotation_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategory_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoco_annotation_dict\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1155\u001b[0m     coco_image\u001b[38;5;241m.\u001b[39madd_annotation(coco_annotation)\n\u001b[0;32m   1156\u001b[0m coco\u001b[38;5;241m.\u001b[39madd_image(coco_image)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_n\\lib\\site-packages\\sahi\\utils\\coco.py:146\u001b[0m, in \u001b[0;36mCocoAnnotation.from_coco_annotation_dict\u001b[1;34m(cls, annotation_dict, category_name)\u001b[0m\n\u001b[0;32m    139\u001b[0m     has_rle_segmentation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    142\u001b[0m     annotation_dict\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m annotation_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_rle_segmentation\n\u001b[0;32m    145\u001b[0m ):\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegmentation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategory_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategory_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategory_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    153\u001b[0m         bbox\u001b[38;5;241m=\u001b[39mannotation_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    154\u001b[0m         category_id\u001b[38;5;241m=\u001b[39mannotation_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    155\u001b[0m         category_name\u001b[38;5;241m=\u001b[39mcategory_name,\n\u001b[0;32m    156\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_n\\lib\\site-packages\\sahi\\utils\\coco.py:221\u001b[0m, in \u001b[0;36mCocoAnnotation.__init__\u001b[1;34m(self, segmentation, bbox, category_id, category_name, image_id, iscrowd)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iscrowd \u001b[38;5;241m=\u001b[39m iscrowd\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_segmentation:\n\u001b[1;32m--> 221\u001b[0m     shapely_annotation \u001b[38;5;241m=\u001b[39m \u001b[43mShapelyAnnotation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_coco_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_segmentation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     shapely_annotation \u001b[38;5;241m=\u001b[39m ShapelyAnnotation\u001b[38;5;241m.\u001b[39mfrom_coco_bbox(bbox\u001b[38;5;241m=\u001b[39mbbox)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_n\\lib\\site-packages\\sahi\\utils\\shapely.py:66\u001b[0m, in \u001b[0;36mShapelyAnnotation.from_coco_segmentation\u001b[1;34m(cls, segmentation, slice_bbox)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_coco_segmentation\u001b[39m(\u001b[38;5;28mcls\u001b[39m, segmentation, slice_bbox\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    Init ShapelyAnnotation from coco segmentation.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m        Is used to calculate sliced coco coordinates.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     shapely_multipolygon \u001b[38;5;241m=\u001b[39m \u001b[43mget_shapely_multipolygon\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmentation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(multipolygon\u001b[38;5;241m=\u001b[39mshapely_multipolygon, slice_bbox\u001b[38;5;241m=\u001b[39mslice_bbox)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_n\\lib\\site-packages\\sahi\\utils\\shapely.py:29\u001b[0m, in \u001b[0;36mget_shapely_multipolygon\u001b[1;34m(coco_segmentation)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m coco_polygon \u001b[38;5;129;01min\u001b[39;00m coco_segmentation:\n\u001b[0;32m     28\u001b[0m     point_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(coco_polygon[\u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m], coco_polygon[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]))\n\u001b[1;32m---> 29\u001b[0m     shapely_polygon \u001b[38;5;241m=\u001b[39m \u001b[43mPolygon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     polygon_list\u001b[38;5;241m.\u001b[39mappend(shapely_polygon)\n\u001b[0;32m     31\u001b[0m shapely_multipolygon \u001b[38;5;241m=\u001b[39m MultiPolygon(polygon_list)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_n\\lib\\site-packages\\shapely\\geometry\\polygon.py:230\u001b[0m, in \u001b[0;36mPolygon.__new__\u001b[1;34m(self, shell, holes)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m shell\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     shell \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m holes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(holes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;66;03m# shapely constructor cannot handle holes=[]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_n\\lib\\site-packages\\shapely\\geometry\\polygon.py:104\u001b[0m, in \u001b[0;36mLinearRing.__new__\u001b[1;34m(self, coordinates)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(coordinates) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# empty geometry\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# TODO better constructor + should shapely.linearrings handle this?\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m shapely\u001b[38;5;241m.\u001b[39mfrom_wkt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLINEARRING EMPTY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m geom \u001b[38;5;241m=\u001b[39m \u001b[43mshapely\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinearrings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(geom, LinearRing):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid values passed to LinearRing constructor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_n\\lib\\site-packages\\shapely\\decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[0;32m     76\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_n\\lib\\site-packages\\shapely\\creation.py:171\u001b[0m, in \u001b[0;36mlinearrings\u001b[1;34m(coords, y, z, indices, out, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m coords \u001b[38;5;241m=\u001b[39m _xyz_to_coords(coords, y, z)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinearrings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m simple_geometries_1d(coords, indices, GeometryType\u001b[38;5;241m.\u001b[39mLINEARRING, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mValueError\u001b[0m: A linearring requires at least 4 coordinates."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_n",
   "language": "python",
   "name": "sam_n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
