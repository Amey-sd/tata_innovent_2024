{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e03f35-ae20-4260-a57d-1bcd882727cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1337\n",
      "e:\\random python scripts\\tata haxs\\sam\\segment-anything\\segment_anything\\build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Epoch: [1][1/189] | Time [22.160s (22.160s)] | Data [9.578s (9.578s)] | Focal Loss [0.0245 (0.0245)] | Dice Loss [0.0334 (0.0334)] | IoU Loss [0.0003 (0.0003)] | Total Loss [0.5232 (0.5232)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Siddharth\\anaconda3\\envs\\sam_l\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"C:\\Users\\Siddharth\\anaconda3\\envs\\sam_l\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Siddharth\\anaconda3\\envs\\sam_l\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"E:\\Random Python Scripts\\Tata HaxS\\sam_l\\dataset.py\", line 42, in __getitem__\n    mask = self.coco.annToMask(ann)\n  File \"C:\\Users\\Siddharth\\anaconda3\\envs\\sam_l\\lib\\site-packages\\pycocotools\\coco.py\", line 442, in annToMask\n    rle = self.annToRLE(ann)\n  File \"C:\\Users\\Siddharth\\anaconda3\\envs\\sam_l\\lib\\site-packages\\pycocotools\\coco.py\", line 427, in annToRLE\n    rles = maskUtils.frPyObjects(segm, h, w)\n  File \"pycocotools\\\\_mask.pyx\", line 295, in pycocotools._mask.frPyObjects\nTypeError: Argument 'bb' has incorrect type (expected numpy.ndarray, got list)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 167\u001b[0m\n\u001b[0;32m    163\u001b[0m     validate(fabric, model, val_data, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 162\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m    159\u001b[0m optimizer, scheduler \u001b[38;5;241m=\u001b[39m configure_opt(cfg, model)\n\u001b[0;32m    160\u001b[0m model, optimizer \u001b[38;5;241m=\u001b[39m fabric\u001b[38;5;241m.\u001b[39msetup(model, optimizer)\n\u001b[1;32m--> 162\u001b[0m \u001b[43mtrain_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfabric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m validate(fabric, model, val_data, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 81\u001b[0m, in \u001b[0;36mtrain_sam\u001b[1;34m(cfg, fabric, model, optimizer, scheduler, train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     78\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     79\u001b[0m validated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39meval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated:\n\u001b[0;32m     83\u001b[0m         validate(fabric, model, val_dataloader, epoch)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_l\\lib\\site-packages\\lightning\\fabric\\wrappers.py:331\u001b[0m, in \u001b[0;36m_FabricDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader:\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m move_data_to_device(item, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_l\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_l\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1324\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1323\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_l\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam_l\\lib\\site-packages\\torch\\_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Siddharth\\anaconda3\\envs\\sam_l\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"C:\\Users\\Siddharth\\anaconda3\\envs\\sam_l\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Siddharth\\anaconda3\\envs\\sam_l\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"E:\\Random Python Scripts\\Tata HaxS\\sam_l\\dataset.py\", line 42, in __getitem__\n    mask = self.coco.annToMask(ann)\n  File \"C:\\Users\\Siddharth\\anaconda3\\envs\\sam_l\\lib\\site-packages\\pycocotools\\coco.py\", line 442, in annToMask\n    rle = self.annToRLE(ann)\n  File \"C:\\Users\\Siddharth\\anaconda3\\envs\\sam_l\\lib\\site-packages\\pycocotools\\coco.py\", line 427, in annToRLE\n    rles = maskUtils.frPyObjects(segm, h, w)\n  File \"pycocotools\\\\_mask.pyx\", line 295, in pycocotools._mask.frPyObjects\nTypeError: Argument 'bb' has incorrect type (expected numpy.ndarray, got list)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import lightning as L\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from box import Box\n",
    "from config import cfg\n",
    "from dataset import load_datasets\n",
    "from lightning.fabric.fabric import _FabricOptimizer\n",
    "from lightning.fabric.loggers import TensorBoardLogger\n",
    "from losses import DiceLoss\n",
    "from losses import FocalLoss\n",
    "from model import Model\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import AverageMeter\n",
    "from utils import calc_iou\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "\n",
    "def validate(fabric: L.Fabric, model: Model, val_dataloader: DataLoader, epoch: int = 0):\n",
    "    model.eval()\n",
    "    ious = AverageMeter()\n",
    "    f1_scores = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for iter, data in enumerate(val_dataloader):\n",
    "            images, bboxes, gt_masks = data\n",
    "            num_images = images.size(0)\n",
    "            pred_masks, _ = model(images, bboxes)\n",
    "            for pred_mask, gt_mask in zip(pred_masks, gt_masks):\n",
    "                batch_stats = smp.metrics.get_stats(\n",
    "                    pred_mask,\n",
    "                    gt_mask.int(),\n",
    "                    mode='binary',\n",
    "                    threshold=0.5,\n",
    "                )\n",
    "                batch_iou = smp.metrics.iou_score(*batch_stats, reduction=\"micro-imagewise\")\n",
    "                batch_f1 = smp.metrics.f1_score(*batch_stats, reduction=\"micro-imagewise\")\n",
    "                ious.update(batch_iou, num_images)\n",
    "                f1_scores.update(batch_f1, num_images)\n",
    "            fabric.print(\n",
    "                f'Val: [{epoch}] - [{iter}/{len(val_dataloader)}]: Mean IoU: [{ious.avg:.4f}] -- Mean F1: [{f1_scores.avg:.4f}]'\n",
    "            )\n",
    "\n",
    "    fabric.print(f'Validation [{epoch}]: Mean IoU: [{ious.avg:.4f}] -- Mean F1: [{f1_scores.avg:.4f}]')\n",
    "\n",
    "    fabric.print(f\"Saving checkpoint to {cfg.out_dir}\")\n",
    "    state_dict = model.model.state_dict()\n",
    "    if fabric.global_rank == 0:\n",
    "        torch.save(state_dict, os.path.join(cfg.out_dir, f\"epoch-{epoch:06d}-f1{f1_scores.avg:.2f}-ckpt.pth\"))\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_sam(\n",
    "    cfg: Box,\n",
    "    fabric: L.Fabric,\n",
    "    model: Model,\n",
    "    optimizer: _FabricOptimizer,\n",
    "    scheduler: _FabricOptimizer,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "):\n",
    "    \"\"\"The SAM training loop.\"\"\"\n",
    "\n",
    "    focal_loss = FocalLoss()\n",
    "    dice_loss = DiceLoss()\n",
    "\n",
    "    for epoch in range(1, cfg.num_epochs):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        focal_losses = AverageMeter()\n",
    "        dice_losses = AverageMeter()\n",
    "        iou_losses = AverageMeter()\n",
    "        total_losses = AverageMeter()\n",
    "        end = time.time()\n",
    "        validated = False\n",
    "\n",
    "        for iter, data in enumerate(train_dataloader):\n",
    "            if epoch > 1 and epoch % cfg.eval_interval == 0 and not validated:\n",
    "                validate(fabric, model, val_dataloader, epoch)\n",
    "                validated = True\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "            images, bboxes, gt_masks = data\n",
    "            batch_size = images.size(0)\n",
    "            pred_masks, iou_predictions = model(images, bboxes)\n",
    "            num_masks = sum(len(pred_mask) for pred_mask in pred_masks)\n",
    "            loss_focal = torch.tensor(0., device=fabric.device)\n",
    "            loss_dice = torch.tensor(0., device=fabric.device)\n",
    "            loss_iou = torch.tensor(0., device=fabric.device)\n",
    "            for pred_mask, gt_mask, iou_prediction in zip(pred_masks, gt_masks, iou_predictions):\n",
    "                batch_iou = calc_iou(pred_mask, gt_mask)\n",
    "                loss_focal += focal_loss(pred_mask, gt_mask, num_masks)\n",
    "                loss_dice += dice_loss(pred_mask, gt_mask, num_masks)\n",
    "                loss_iou += F.mse_loss(iou_prediction, batch_iou, reduction='sum') / num_masks\n",
    "\n",
    "            loss_total = 20. * loss_focal + loss_dice + loss_iou\n",
    "            optimizer.zero_grad()\n",
    "            fabric.backward(loss_total)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            focal_losses.update(loss_focal.item(), batch_size)\n",
    "            dice_losses.update(loss_dice.item(), batch_size)\n",
    "            iou_losses.update(loss_iou.item(), batch_size)\n",
    "            total_losses.update(loss_total.item(), batch_size)\n",
    "\n",
    "            fabric.print(f'Epoch: [{epoch}][{iter+1}/{len(train_dataloader)}]'\n",
    "                         f' | Time [{batch_time.val:.3f}s ({batch_time.avg:.3f}s)]'\n",
    "                         f' | Data [{data_time.val:.3f}s ({data_time.avg:.3f}s)]'\n",
    "                         f' | Focal Loss [{focal_losses.val:.4f} ({focal_losses.avg:.4f})]'\n",
    "                         f' | Dice Loss [{dice_losses.val:.4f} ({dice_losses.avg:.4f})]'\n",
    "                         f' | IoU Loss [{iou_losses.val:.4f} ({iou_losses.avg:.4f})]'\n",
    "                         f' | Total Loss [{total_losses.val:.4f} ({total_losses.avg:.4f})]')\n",
    "\n",
    "\n",
    "def configure_opt(cfg: Box, model: Model):\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < cfg.opt.warmup_steps:\n",
    "            return step / cfg.opt.warmup_steps\n",
    "        elif step < cfg.opt.steps[0]:\n",
    "            return 1.0\n",
    "        elif step < cfg.opt.steps[1]:\n",
    "            return 1 / cfg.opt.decay_factor\n",
    "        else:\n",
    "            return 1 / (cfg.opt.decay_factor**2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.model.parameters(), lr=cfg.opt.learning_rate, weight_decay=cfg.opt.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "def main(cfg: Box) -> None:\n",
    "    fabric = L.Fabric(accelerator=\"auto\",\n",
    "                      devices=cfg.num_devices,\n",
    "                      strategy=\"auto\",\n",
    "                      loggers=[TensorBoardLogger(cfg.out_dir, name=\"lightning-sam\")])\n",
    "    fabric.launch()\n",
    "    fabric.seed_everything(1337 + fabric.global_rank)\n",
    "\n",
    "    if fabric.global_rank == 0:\n",
    "        os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "    with fabric.device:\n",
    "        model = Model(cfg)\n",
    "        model.setup()\n",
    "\n",
    "    train_data, val_data = load_datasets(cfg, model.model.image_encoder.img_size)\n",
    "    train_data = fabric._setup_dataloader(train_data)\n",
    "    val_data = fabric._setup_dataloader(val_data)\n",
    "\n",
    "    optimizer, scheduler = configure_opt(cfg, model)\n",
    "    model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "    train_sam(cfg, fabric, model, optimizer, scheduler, train_data, val_data)\n",
    "    validate(fabric, model, val_data, epoch=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_l",
   "language": "python",
   "name": "sam_l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
